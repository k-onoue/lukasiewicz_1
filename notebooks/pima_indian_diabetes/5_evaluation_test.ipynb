{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/keisukeonoue/ws/lukasiewicz_1/notebooks/pima_indian_diabetes\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "# project_dir_path = '/home/onoue/ws/lukasiewicz_1'\n",
    "project_dir_path = '/Users/keisukeonoue/ws/lukasiewicz_1/'\n",
    "sys.path.append(project_dir_path)\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from src.setup_problem_primal_modular import Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1     -1\n",
       "2      1\n",
       "3     -1\n",
       "5     -1\n",
       "      ..\n",
       "763   -1\n",
       "764   -1\n",
       "765   -1\n",
       "766    1\n",
       "767   -1\n",
       "Name: Outcome, Length: 670, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/diabetes_cleaned.csv', index_col=0)\n",
    "X = data.drop('Outcome', axis=1)\n",
    "\n",
    "#####################################################3\n",
    "y = data['Outcome']\n",
    "y.replace(0, -1, inplace=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any, List\n",
    "import json\n",
    "\n",
    "# from .setup_problem import Setup\n",
    "class Setup_:\n",
    "    \"\"\"\n",
    "    型ヒント用（circular import の回避のため）\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from src.misc import is_symbol\n",
    "from src.operators import negation\n",
    "\n",
    "\n",
    "\n",
    "class EvaluateModel:\n",
    "    def __init__(self,\n",
    "                 path_cleaned: str,\n",
    "                 path_discretized: str,\n",
    "                 model: object,\n",
    "                 KB_origin: List[List[str]],\n",
    "                 random_state: int = 42,\n",
    "                 test_size: float = 0.2,\n",
    "                 name: str = None,\n",
    "                 note: str = None) -> None:\n",
    "\n",
    "        self.path_cleaned = path_cleaned\n",
    "        self.path_discretized = path_discretized\n",
    "        self.model = model\n",
    "        self.KB_origin = KB_origin\n",
    "        self.random_state = random_state \n",
    "        self.test_size = test_size\n",
    "\n",
    "        self.result_dict = {\n",
    "            'name'     : name,\n",
    "            'note'     : note,\n",
    "            'Accuracy' : None,\n",
    "            'Precision': None,\n",
    "            'Recall'   : None,\n",
    "            'F1-score' : None,\n",
    "            'Auc'      : None,\n",
    "            'len_U': None,\n",
    "            'Rules'    : {'violation': 0, 'total': len(self.KB_origin)},\n",
    "            'Rules_detail': {}\n",
    "        }\n",
    "\n",
    "    def calculate_scores(self) -> None:\n",
    "        # まずは連続データで計算\n",
    "        data = pd.read_csv(self.path_cleaned, index_col=0)\n",
    "        X = data.drop('Outcome', axis=1)\n",
    "\n",
    "        #####################################################3\n",
    "        y = data['Outcome']\n",
    "        y.replace(0, -1, inplace=True)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=self.test_size, \n",
    "                                                            random_state=self.random_state)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        y_pred_interpreted = np.where(y_pred == 0, -1, y_pred)\n",
    "        \n",
    "        y_pred = self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 精度等の一般的な評価指標の計算\n",
    "        accuracy = accuracy_score(y_test, y_pred_interpreted)\n",
    "        # conf_matrix = confusion_matrix(y_test, y_pred_interpreted)\n",
    "        precision = precision_score(y_test, y_pred_interpreted)\n",
    "        recall = recall_score(y_test, y_pred_interpreted)\n",
    "        f1 = f1_score(y_test, y_pred_interpreted)\n",
    "        # class_report = classification_report(y_test, y_pred_interpreted)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        self.result_dict['Accuracy'] = float(accuracy)\n",
    "        # self.result_dict['Confusion_matrix'] = conf_matrix.tolist()\n",
    "        self.result_dict['Precision'] = float(precision)\n",
    "        self.result_dict['Recall'] = float(recall)\n",
    "        self.result_dict['F1-score'] = float(f1)\n",
    "        # self.result_dict['Classification_report'] = class_report\n",
    "        self.result_dict['Auc'] = float(roc_auc)\n",
    "\n",
    "        # ルール違反の計算の前に X_test を離散化（discretized のほうを読み込む）\n",
    "        data = pd.read_csv(self.path_discretized, index_col=0)\n",
    "\n",
    "        X = data.drop('Outcome', axis=1)\n",
    "\n",
    "    \n",
    "        ###########################################################\n",
    "        y = data['Outcome']\n",
    "        y.replace(0, -1, inplace=True)\n",
    "\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=self.test_size, \n",
    "                                                            random_state=self.random_state)\n",
    "        \n",
    "        # ルール違反\n",
    "        rules_tmp = []\n",
    "        for rule in self.KB_origin:\n",
    "            if \"Outcome\" in rule:\n",
    "                tmp = {}\n",
    "                for idx, item in enumerate(rule):\n",
    "                    if not is_symbol(item):\n",
    "                        if idx == 0 or rule[idx - 1] != '¬':\n",
    "                            tmp[item] = 1\n",
    "                        elif item != \"Outcome\":\n",
    "                            tmp[item] = 0\n",
    "                        else:\n",
    "                            tmp[item] = -1\n",
    "                rules_tmp.append(tmp)\n",
    "\n",
    "        idx_tmp = X_test.index\n",
    "        y_pred_interpreted = pd.DataFrame(y_pred_interpreted, index=idx_tmp)\n",
    "        \n",
    "\n",
    "        for i, rule in enumerate(rules_tmp):\n",
    "            outcome = rule[\"Outcome\"]\n",
    "            condition = \" & \".join([f\"{column} == {value}\" for column, value in rule.items() if column != \"Outcome\"])\n",
    "\n",
    "            tmp = y_pred_interpreted.loc[X_test.query(condition).index]\n",
    "\n",
    "            violation_tmp = int((tmp != outcome).sum().iloc[0])\n",
    "            self.result_dict['Rules']['violation'] += violation_tmp\n",
    "            self.result_dict['Rules']['total'] += tmp.shape[0]\n",
    "            self.result_dict['Rules_detail'][i] = {\n",
    "                'violation': violation_tmp,\n",
    "                'total': tmp.shape[0]\n",
    "            }\n",
    "\n",
    "    def save_result_as_json(self, file_path) -> None:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.result_dict, f, indent=4)\n",
    "\n",
    "    def evaluate(self, save_file_path: str = './result_1.json') -> None:\n",
    "        self.calculate_scores()\n",
    "        self.save_result_as_json(file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data took 0.10036587715148926 seconds!\n",
      "load_rules took 0.0002872943878173828 seconds!\n"
     ]
    }
   ],
   "source": [
    "# KB_origin の生成\n",
    "data_dir_path = os.path.join(project_dir_path, 'inputs/pima_indian_diabetes')\n",
    "file_list = os.listdir(os.path.join(data_dir_path, 'train'))\n",
    "\n",
    "L_files = [filename for filename in file_list \n",
    "           if filename.startswith('L') and filename.endswith('.csv')]\n",
    "\n",
    "U_files = [filename for filename in file_list \n",
    "           if filename.startswith('U') and filename.endswith('.csv')]\n",
    "\n",
    "file_names_dict = {\n",
    "    'supervised': L_files,\n",
    "    'unsupervised': U_files,\n",
    "    'rule': ['rules.txt']\n",
    "}\n",
    "\n",
    "problem_instance = Setup(data_dir_path, \n",
    "                         file_names_dict, \n",
    "                         specimen_construct_objective_function)\n",
    "\n",
    "problem_instance.load_data()\n",
    "problem_instance.load_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non-linear svm (rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cleaned = \"./data/diabetes_cleaned.csv\"\n",
    "path_discretized = \"./data/diabetes_discretized.csv\"\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "model = SVC(kernel='rbf',\n",
    "            random_state=random_state,\n",
    "            probability=True) \n",
    "\n",
    "\n",
    "#########################################\n",
    "save_file_path = \"./result_3.json\"\n",
    "model_name = \"non-linear svm (rbf)\"\n",
    "note = None\n",
    "\n",
    "evaluate_model = EvaluateModel(path_cleaned=path_cleaned,\n",
    "                               path_discretized=path_discretized,\n",
    "                               model=model,\n",
    "                               name=model_name,\n",
    "                               random_state=random_state,\n",
    "                               test_size=test_size,\n",
    "                               KB_origin=problem_instance.KB_origin)\n",
    "\n",
    "evaluate_model.evaluate(save_file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cleaned = \"./data/diabetes_cleaned.csv\"\n",
    "path_discretized = \"./data/diabetes_discretized.csv\"\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "model = SVC(kernel='linear',\n",
    "            random_state=random_state,\n",
    "            probability=True) \n",
    "\n",
    "\n",
    "#########################################\n",
    "save_file_path = \"./result_4.json\"\n",
    "model_name = \"linear svm\"\n",
    "note = None\n",
    "\n",
    "evaluate_model = EvaluateModel(path_cleaned=path_cleaned,\n",
    "                               path_discretized=path_discretized,\n",
    "                               model=model,\n",
    "                               name=model_name,\n",
    "                               random_state=random_state,\n",
    "                               test_size=test_size,\n",
    "                               KB_origin=problem_instance.KB_origin)\n",
    "\n",
    "evaluate_model.evaluate(save_file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onoue/ws/lukasiewicz_1/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "path_cleaned = \"./data/diabetes_cleaned.csv\"\n",
    "path_discretized = \"./data/diabetes_discretized.csv\"\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "model = SVC(kernel='linear',\n",
    "            random_state=random_state,\n",
    "            probability=True) \n",
    "model = LogisticRegression(random_state=random_state)\n",
    "\n",
    "#########################################\n",
    "save_file_path = \"./result_5.json\"\n",
    "model_name = \"logistic regression\"\n",
    "note = None\n",
    "\n",
    "evaluate_model = EvaluateModel(path_cleaned=path_cleaned,\n",
    "                               path_discretized=path_discretized,\n",
    "                               model=model,\n",
    "                               name=model_name,\n",
    "                               test_size=test_size,\n",
    "                               random_state=random_state,\n",
    "                               KB_origin=problem_instance.KB_origin)\n",
    "\n",
    "evaluate_model.evaluate(save_file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cleaned = \"./data/diabetes_cleaned.csv\"\n",
    "path_discretized = \"./data/diabetes_discretized.csv\"\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "model = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "\n",
    "#########################################\n",
    "save_file_path = \"./result_6.json\"\n",
    "model_name = \"random forest\"\n",
    "note = None\n",
    "\n",
    "evaluate_model = EvaluateModel(path_cleaned=path_cleaned,\n",
    "                               path_discretized=path_discretized,\n",
    "                               model=model,\n",
    "                               name=model_name,\n",
    "                               random_state=random_state,\n",
    "                               test_size=test_size,\n",
    "                               KB_origin=problem_instance.KB_origin)\n",
    "\n",
    "evaluate_model.evaluate(save_file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
