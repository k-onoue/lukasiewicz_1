{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "# project_dir_path = '/home/onoue/ws/lukasiewicz_1'\n",
    "project_dir_path = '/Users/keisukeonoue/ws/lukasiewicz_1/'\n",
    "sys.path.append(project_dir_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from src.rulefit import RuleFitClassifier\n",
    "from src.setup_problem_primal_modular import Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RuleFitClassifier のなかの tree generator (random forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any, List\n",
    "import json\n",
    "\n",
    "# from .setup_problem import Setup\n",
    "class Setup_:\n",
    "    \"\"\"\n",
    "    型ヒント用（circular import の回避のため）\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from src.misc import is_symbol\n",
    "from src.operators import negation\n",
    "\n",
    "\n",
    "\n",
    "class EvaluateModelRuleFit:\n",
    "    def __init__(self,\n",
    "                 path_discretized: str,\n",
    "                 model: object,\n",
    "                 KB_origin: List[List[str]],\n",
    "                 random_state: int = 42,\n",
    "                 test_size: float = 0.2,\n",
    "                 name: str = None,\n",
    "                 note: str = None) -> None:\n",
    "\n",
    "        self.path_discretized = path_discretized\n",
    "        self.model = model\n",
    "        self.KB_origin = KB_origin\n",
    "        self.random_state = random_state \n",
    "        self.test_size = test_size\n",
    "\n",
    "        self.result_dict = {\n",
    "            'name'     : name,\n",
    "            'note'     : note,\n",
    "            'Accuracy' : None,\n",
    "            'Precision': None,\n",
    "            'Recall'   : None,\n",
    "            'F1-score' : None,\n",
    "            'Auc'      : None,\n",
    "            'len_U'    : None,\n",
    "            'Rules'    : {'violation': 0, 'total': len(self.KB_origin)},\n",
    "            'Rules_detail': {}\n",
    "        }\n",
    "\n",
    "    def calculate_scores(self) -> None:\n",
    "        data = pd.read_csv(self.path_discretized, index_col=0)\n",
    "        X = data.drop('Outcome', axis=1)\n",
    "\n",
    "        #####################################################3\n",
    "        y = data['Outcome']\n",
    "        y.replace(0, -1, inplace=True)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=self.test_size, \n",
    "                                                            random_state=self.random_state)\n",
    "\n",
    "        idx_tmp = X_test.index\n",
    "\n",
    "        feature_names = list(X_train.columns)\n",
    "        X_train = X_train.values\n",
    "        X_test  = X_test.values\n",
    "        y_train = y_train.values\n",
    "        y_test  = y_test.values        \n",
    "        self.model.fit(X_train, y_train, feature_names=feature_names)\n",
    "\n",
    "\n",
    "        y_pred = self.model.tree_generator.predict(X_test)\n",
    "\n",
    "        y_pred_interpreted = np.where(y_pred == 0, -1, y_pred)\n",
    "        \n",
    "        y_pred = self.model.tree_generator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 精度等の一般的な評価指標の計算\n",
    "        accuracy = accuracy_score(y_test, y_pred_interpreted)\n",
    "        # conf_matrix = confusion_matrix(y_test, y_pred_interpreted)\n",
    "        precision = precision_score(y_test, y_pred_interpreted)\n",
    "        recall = recall_score(y_test, y_pred_interpreted)\n",
    "        f1 = f1_score(y_test, y_pred_interpreted)\n",
    "        # class_report = classification_report(y_test, y_pred_interpreted)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        self.result_dict['Accuracy'] = float(accuracy)\n",
    "        # self.result_dict['Confusion_matrix'] = conf_matrix.tolist()\n",
    "        self.result_dict['Precision'] = float(precision)\n",
    "        self.result_dict['Recall'] = float(recall)\n",
    "        self.result_dict['F1-score'] = float(f1)\n",
    "        # self.result_dict['Classification_report'] = class_report\n",
    "        self.result_dict['Auc'] = float(roc_auc)\n",
    "        \n",
    "        # ルール違反\n",
    "        rules_tmp = []\n",
    "        for rule in self.KB_origin:\n",
    "            if \"Outcome\" in rule:\n",
    "                tmp = {}\n",
    "                for idx, item in enumerate(rule):\n",
    "                    if not is_symbol(item):\n",
    "                        if idx == 0 or rule[idx - 1] != '¬':\n",
    "                            tmp[item] = 1\n",
    "                        elif item != \"Outcome\":\n",
    "                            tmp[item] = 0\n",
    "                        else:\n",
    "                            tmp[item] = -1\n",
    "                rules_tmp.append(tmp)\n",
    "\n",
    "        \n",
    "        X_test = pd.DataFrame(X_test, columns=feature_names, index=idx_tmp)\n",
    "        y_pred_interpreted = pd.DataFrame(y_pred_interpreted, index=idx_tmp)\n",
    "        \n",
    "\n",
    "        for i, rule in enumerate(rules_tmp):\n",
    "            outcome = rule[\"Outcome\"]\n",
    "            condition = \" & \".join([f\"{column} == {value}\" for column, value in rule.items() if column != \"Outcome\"])\n",
    "\n",
    "            tmp = y_pred_interpreted.loc[X_test.query(condition).index]\n",
    "\n",
    "            violation_bool = 1 if int((tmp != outcome).sum().iloc[0]) >= 1 else 0\n",
    "            self.result_dict['Rules']['violation'] += violation_bool\n",
    "            self.result_dict['Rules_detail'][i] = {\n",
    "                'rule': \" \".join(self.KB_origin[i]),\n",
    "                'violation': violation_bool,\n",
    "            }\n",
    "\n",
    "    def save_result_as_json(self, file_path) -> None:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.result_dict, f, indent=4)\n",
    "\n",
    "    def evaluate(self, save_file_path: str = './result_1.json') -> None:\n",
    "        self.calculate_scores()\n",
    "        self.save_result_as_json(file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_rules took 0.00028896331787109375 seconds!\n"
     ]
    }
   ],
   "source": [
    "# KB_origin の生成\n",
    "data_dir_path = os.path.join(project_dir_path, 'inputs/pima_indian_diabetes')\n",
    "file_list = os.listdir(os.path.join(data_dir_path, 'train'))\n",
    "\n",
    "L_files = [filename for filename in file_list \n",
    "           if filename.startswith('L') and filename.endswith('.csv')]\n",
    "\n",
    "U_files = [filename for filename in file_list \n",
    "           if filename.startswith('U') and filename.endswith('.csv')]\n",
    "\n",
    "file_names_dict = {\n",
    "    'supervised': L_files,\n",
    "    'unsupervised': U_files,\n",
    "    'rule': ['rules.txt']\n",
    "}\n",
    "\n",
    "problem_instance = Setup(data_dir_path, \n",
    "                         file_names_dict, \n",
    "                         None)\n",
    "\n",
    "problem_instance.load_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_discretized = \"./data/diabetes_discretized.csv\"\n",
    "\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "rfmode = 'classify'\n",
    "tree_generator = RandomForestClassifier(random_state=random_state)\n",
    "# ここでの random forest に対する seed の設定は意味がない\n",
    "\n",
    "model = RuleFitClassifier(rfmode=rfmode,\n",
    "                          tree_generator=tree_generator,\n",
    "                          random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "save_file_path = \"./../../outputs/pima_indian_diabetes/result_rulefit_1.json\"\n",
    "model_name = \"random forest (rulefit)\"\n",
    "note = None\n",
    "\n",
    "evaluate_model = EvaluateModelRuleFit(path_discretized=path_discretized,\n",
    "                               model=model,\n",
    "                               name=model_name,\n",
    "                               random_state=random_state,\n",
    "                               test_size=test_size,\n",
    "                               KB_origin=problem_instance.KB_origin)\n",
    "\n",
    "evaluate_model.evaluate(save_file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RuleFitClassifier そのもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateModelRuleFit2:\n",
    "    def __init__(self,\n",
    "                 path_discretized: str,\n",
    "                 model: object,\n",
    "                 KB_origin: List[List[str]],\n",
    "                 random_state: int = 42,\n",
    "                 test_size: float = 0.2,\n",
    "                 name: str = None,\n",
    "                 note: str = None) -> None:\n",
    "\n",
    "        self.path_discretized = path_discretized\n",
    "        self.model = model\n",
    "        self.KB_origin = KB_origin\n",
    "        self.random_state = random_state \n",
    "        self.test_size = test_size\n",
    "\n",
    "        self.result_dict = {\n",
    "            'name'     : name,\n",
    "            'note'     : note,\n",
    "            'Accuracy' : None,\n",
    "            'Precision': None,\n",
    "            'Recall'   : None,\n",
    "            'F1-score' : None,\n",
    "            'Auc'      : None,\n",
    "            'len_U'    : None,\n",
    "            'Rules'    : {'violation': 0, 'total': len(self.KB_origin)},\n",
    "            'Rules_detail': {}\n",
    "        }\n",
    "\n",
    "    def calculate_scores(self) -> None:\n",
    "        data = pd.read_csv(self.path_discretized, index_col=0)\n",
    "        X = data.drop('Outcome', axis=1)\n",
    "\n",
    "        #####################################################3\n",
    "        y = data['Outcome']\n",
    "        y.replace(0, -1, inplace=True)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=self.test_size, \n",
    "                                                            random_state=self.random_state)\n",
    "        \n",
    "\n",
    "        idx_tmp = X_test.index\n",
    "\n",
    "        feature_names = list(X_train.columns)\n",
    "        X_train = X_train.values\n",
    "        X_test  = X_test.values\n",
    "        y_train = y_train.values\n",
    "        y_test  = y_test.values        \n",
    "        self.model.fit(X_train, y_train, feature_names=feature_names)\n",
    "\n",
    "\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        y_pred_interpreted = np.where(y_pred == 0, -1, y_pred)\n",
    "        \n",
    "        y_pred = self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 精度等の一般的な評価指標の計算\n",
    "        accuracy = accuracy_score(y_test, y_pred_interpreted)\n",
    "        precision = precision_score(y_test, y_pred_interpreted)\n",
    "        recall = recall_score(y_test, y_pred_interpreted)\n",
    "        f1 = f1_score(y_test, y_pred_interpreted)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        self.result_dict['Accuracy'] = float(accuracy)\n",
    "        self.result_dict['Precision'] = float(precision)\n",
    "        self.result_dict['Recall'] = float(recall)\n",
    "        self.result_dict['F1-score'] = float(f1)\n",
    "        self.result_dict['Auc'] = float(roc_auc)\n",
    "        \n",
    "        # ルール違反\n",
    "        rules_tmp = []\n",
    "        for rule in self.KB_origin:\n",
    "            if \"Outcome\" in rule:\n",
    "                tmp = {}\n",
    "                for idx, item in enumerate(rule):\n",
    "                    if not is_symbol(item):\n",
    "                        if idx == 0 or rule[idx - 1] != '¬':\n",
    "                            tmp[item] = 1\n",
    "                        elif item != \"Outcome\":\n",
    "                            tmp[item] = 0\n",
    "                        else:\n",
    "                            tmp[item] = -1\n",
    "                rules_tmp.append(tmp)\n",
    "\n",
    "        \n",
    "        X_test = pd.DataFrame(X_test, columns=feature_names, index=idx_tmp)\n",
    "        y_pred_interpreted = pd.DataFrame(y_pred_interpreted, index=idx_tmp)\n",
    "        \n",
    "\n",
    "        for i, rule in enumerate(rules_tmp):\n",
    "            outcome = rule[\"Outcome\"]\n",
    "            condition = \" & \".join([f\"{column} == {value}\" for column, value in rule.items() if column != \"Outcome\"])\n",
    "\n",
    "            tmp = y_pred_interpreted.loc[X_test.query(condition).index]\n",
    "\n",
    "            violation_bool = 1 if int((tmp != outcome).sum().iloc[0]) >= 1 else 0\n",
    "            self.result_dict['Rules']['violation'] += violation_bool\n",
    "            self.result_dict['Rules_detail'][i] = {\n",
    "                'rule': \" \".join(self.KB_origin[i]),\n",
    "                'violation': violation_bool,\n",
    "            }\n",
    "\n",
    "    def save_result_as_json(self, file_path) -> None:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.result_dict, f, indent=4)\n",
    "\n",
    "    def evaluate(self, save_file_path: str = './result_1.json') -> None:\n",
    "        self.calculate_scores()\n",
    "        self.save_result_as_json(file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_discretized = \"./data/diabetes_discretized.csv\"\n",
    "\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "rfmode = 'classify'\n",
    "tree_generator = RandomForestClassifier(random_state=random_state)\n",
    "# ここでの random forest に対する seed の設定は意味がない\n",
    "\n",
    "model = RuleFitClassifier(rfmode=rfmode,\n",
    "                          tree_generator=tree_generator,\n",
    "                          random_state=random_state)\n",
    "\n",
    "#########################################\n",
    "save_file_path = \"./../../outputs/pima_indian_diabetes/result_rulefit_2.json\"\n",
    "model_name = \"RuleFitClassifier\"\n",
    "note = None\n",
    "\n",
    "evaluate_model = EvaluateModelRuleFit2(path_discretized=path_discretized,\n",
    "                               model=model,\n",
    "                               name=model_name,\n",
    "                               random_state=random_state,\n",
    "                               test_size=test_size,\n",
    "                               KB_origin=problem_instance.KB_origin)\n",
    "\n",
    "evaluate_model.evaluate(save_file_path=save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
